{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python project skeleton \u00b6 A cookiecutter template for python projects. Docs : https://marcohenriques.github.io/python-project-skeleton Code : https://github.com/marcohenriques/python-project-skeleton Features included \u00b6 GitHub actions CICD to run formatters, linters, and tests. Also support to build and push docker images Dockerfile to ship python apps Documentation with mkdocs using the beautiful material theme Testing using pytest and several plugins Code formatters using ruff and sqlfluff (SQL) Linters using ruff (check configuration file for enabled plugins), mypy , shellcheck and sqlfluff Python dependencies vulnerabilities scanner using safety pre-commit hooks for some validations Makefile to automate some development tasks poetry to manage your python dependencies Python package pre-configured with: logging using loguru setting module using pydantic to help manage your project settings (optional) CLI example using typer Requirements \u00b6 You\u2019ll need to have cookiecutter installed. Installation \u00b6 Run the following command to create a new project, on your current directory: cookiecutter gh:marcohenriques/python-project-skeleton If you want to use a different version of the template, use the following command: cookiecutter gh:marcohenriques/python-project-skeleton -c <VERSION> where <VERSION> can be the branch, tag or commit of the template repo. Template inputs \u00b6 The template asks for the following inputs: project_name : The name of the project. This is used to name the project folder. package_name : The name of the package. This is used to name the package folder. project_description : A short description of the project. author_name : The name of the author. author_email : The email of the author. github_username_or_org_name : The GitHub username or organization name. python_version : The python version to use. include_docker : Whether to include docker support. include_notebooks : Whether to include support for jupyter notebooks. include_docs : Whether to include support for documentation. include_cli : Whether to include support for a command line interface.","title":"python-project-skeleton"},{"location":"#python-project-skeleton","text":"A cookiecutter template for python projects. Docs : https://marcohenriques.github.io/python-project-skeleton Code : https://github.com/marcohenriques/python-project-skeleton","title":"Python project skeleton"},{"location":"#features-included","text":"GitHub actions CICD to run formatters, linters, and tests. Also support to build and push docker images Dockerfile to ship python apps Documentation with mkdocs using the beautiful material theme Testing using pytest and several plugins Code formatters using ruff and sqlfluff (SQL) Linters using ruff (check configuration file for enabled plugins), mypy , shellcheck and sqlfluff Python dependencies vulnerabilities scanner using safety pre-commit hooks for some validations Makefile to automate some development tasks poetry to manage your python dependencies Python package pre-configured with: logging using loguru setting module using pydantic to help manage your project settings (optional) CLI example using typer","title":"Features included"},{"location":"#requirements","text":"You\u2019ll need to have cookiecutter installed.","title":"Requirements"},{"location":"#installation","text":"Run the following command to create a new project, on your current directory: cookiecutter gh:marcohenriques/python-project-skeleton If you want to use a different version of the template, use the following command: cookiecutter gh:marcohenriques/python-project-skeleton -c <VERSION> where <VERSION> can be the branch, tag or commit of the template repo.","title":"Installation"},{"location":"#template-inputs","text":"The template asks for the following inputs: project_name : The name of the project. This is used to name the project folder. package_name : The name of the package. This is used to name the package folder. project_description : A short description of the project. author_name : The name of the author. author_email : The email of the author. github_username_or_org_name : The GitHub username or organization name. python_version : The python version to use. include_docker : Whether to include docker support. include_notebooks : Whether to include support for jupyter notebooks. include_docs : Whether to include support for documentation. include_cli : Whether to include support for a command line interface.","title":"Template inputs"},{"location":"changelog/","text":"Changelog \u00b6 0.4.0 (2024-04-07) \u00b6 Refactor \u00b6 reorganize project (#6) 0.3.0 (2023-07-22) \u00b6 Feat \u00b6 move to ruff Fix \u00b6 cicd : docker Refactor \u00b6 cicd and configs (#5) 0.2.0 (2022-12-24) \u00b6 Feat \u00b6 use poetry version 1.3 v0.1.0 (2022-10-25) \u00b6 First version of template","title":"Release Notes"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#040-2024-04-07","text":"","title":"0.4.0 (2024-04-07)"},{"location":"changelog/#refactor","text":"reorganize project (#6)","title":"Refactor"},{"location":"changelog/#030-2023-07-22","text":"","title":"0.3.0 (2023-07-22)"},{"location":"changelog/#feat","text":"move to ruff","title":"Feat"},{"location":"changelog/#fix","text":"cicd : docker","title":"Fix"},{"location":"changelog/#refactor_1","text":"cicd and configs (#5)","title":"Refactor"},{"location":"changelog/#020-2022-12-24","text":"","title":"0.2.0 (2022-12-24)"},{"location":"changelog/#feat_1","text":"use poetry version 1.3","title":"Feat"},{"location":"changelog/#v010-2022-10-25","text":"First version of template","title":"v0.1.0 (2022-10-25)"},{"location":"quickstart/","text":"Quickstart \u00b6 Before we start generating our project, make sure you already have everything needed. Generate project \u00b6 Run the following command to create a new project, on your current directory: cookiecutter gh:marcohenriques/python-project-skeleton During this process you\u2019ll be prompted for several inputs to configure your project. Setup new generated project \u00b6 First, go to your project directory: cd <my_project_name> Then, to install all the dependencies just run: make install To make sure everything is ok, you can run: make test If all the tests passed without issues, you\u2019re good to go \ud83d\ude80","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Before we start generating our project, make sure you already have everything needed.","title":"Quickstart"},{"location":"quickstart/#generate-project","text":"Run the following command to create a new project, on your current directory: cookiecutter gh:marcohenriques/python-project-skeleton During this process you\u2019ll be prompted for several inputs to configure your project.","title":"Generate project"},{"location":"quickstart/#setup-new-generated-project","text":"First, go to your project directory: cd <my_project_name> Then, to install all the dependencies just run: make install To make sure everything is ok, you can run: make test If all the tests passed without issues, you\u2019re good to go \ud83d\ude80","title":"Setup new generated project"},{"location":"requirements/","text":"Requirements \u00b6 Rendering project \u00b6 To use this template the only requirement is cookiecutter . Generated project \u00b6 Make Pyenv Poetry In Linux, make sure you have all required Python dependencies installed: sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev liblzma-dev tk-dev To confirm these system dependencies are configured correctly, on your project directory run: ./scripts/verchew After running this, you should see something similar to: Checking for Make... $ make --version GNU Make 3.81 \u2714 MATCHED: GNU Make Checking for Python... $ python --version Python 3.10.5 \u2714 MATCHED: <anything> Checking for Poetry... $ poetry --version Poetry (version 1.2.2) \u2714 MATCHED: 1.2 Checking for Pyenv... $ pyenv --version pyenv 2.3.5 \u2714 MATCHED: <anything> Results: \u2714 \u2714 \u2714 \u2714 If you have all the \u2714 you\u2019re good to go.","title":"Requirements"},{"location":"requirements/#requirements","text":"","title":"Requirements"},{"location":"requirements/#rendering-project","text":"To use this template the only requirement is cookiecutter .","title":"Rendering project"},{"location":"requirements/#generated-project","text":"Make Pyenv Poetry In Linux, make sure you have all required Python dependencies installed: sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev liblzma-dev tk-dev To confirm these system dependencies are configured correctly, on your project directory run: ./scripts/verchew After running this, you should see something similar to: Checking for Make... $ make --version GNU Make 3.81 \u2714 MATCHED: GNU Make Checking for Python... $ python --version Python 3.10.5 \u2714 MATCHED: <anything> Checking for Poetry... $ poetry --version Poetry (version 1.2.2) \u2714 MATCHED: 1.2 Checking for Pyenv... $ pyenv --version pyenv 2.3.5 \u2714 MATCHED: <anything> Results: \u2714 \u2714 \u2714 \u2714 If you have all the \u2714 you\u2019re good to go.","title":"Generated project"},{"location":"tools_included/docker/","text":"Docker \u00b6 The project comes with a Dockerfile to helps containerize your application. It already includes some best practices when building docker images, as such: multi-stage building : to better organize and optimize your build default non-root user when launching your container build from python:<python_version>-slim to keep your image small uses a entrypoint for your container script in docker/entrypoint.sh , modify it according to your needs entrypoint script is called with tini protects you from software that accidentally creates zombie processes ensures that the default signal handlers work for the software you run in your Docker image The Dockerfile uses 2 targets: development : image to use in development. With this you can mount a local volume that points to your code and if you change it locally, it will automatically change in the container. production : image to use in production When building both target, you\u2019ll need to have your project requirements.txt generated beforehand. You can do this running: poetry export -f requirements.txt --without-hashes -o requirements.txt # (1) If you need additional dependency groups add the extra parameter --with <your_other_group_1>,<your_other_group_2> For the development target, you\u2019ll also need your requirements-dev.txt , to generate this run: poetry export -f requirements.txt --without-hashes -o requirements-dev.txt --with dev,tests # (1) If you need additional dependency groups add them like --with dev,tests,<your_other_group> For the production , there\u2019s a Make target that will automatically build your image: make build-docker And other to run it: make run-docker","title":"Docker"},{"location":"tools_included/docker/#docker","text":"The project comes with a Dockerfile to helps containerize your application. It already includes some best practices when building docker images, as such: multi-stage building : to better organize and optimize your build default non-root user when launching your container build from python:<python_version>-slim to keep your image small uses a entrypoint for your container script in docker/entrypoint.sh , modify it according to your needs entrypoint script is called with tini protects you from software that accidentally creates zombie processes ensures that the default signal handlers work for the software you run in your Docker image The Dockerfile uses 2 targets: development : image to use in development. With this you can mount a local volume that points to your code and if you change it locally, it will automatically change in the container. production : image to use in production When building both target, you\u2019ll need to have your project requirements.txt generated beforehand. You can do this running: poetry export -f requirements.txt --without-hashes -o requirements.txt # (1) If you need additional dependency groups add the extra parameter --with <your_other_group_1>,<your_other_group_2> For the development target, you\u2019ll also need your requirements-dev.txt , to generate this run: poetry export -f requirements.txt --without-hashes -o requirements-dev.txt --with dev,tests # (1) If you need additional dependency groups add them like --with dev,tests,<your_other_group> For the production , there\u2019s a Make target that will automatically build your image: make build-docker And other to run it: make run-docker","title":"Docker"},{"location":"tools_included/documentation/","text":"Documentation \u00b6 For building documentation we use mkdocs with the material theme. The material theme already includes several plugins and customizations for mkdocs , you can check the material reference and the PyMdown Extensions on how to get your documentation to the next level \ud83d\ude80. We also packed it with mkdocstrings which allows you automatically create documentation from your source code. The mkdocs.yml file already comes pre-filled with some plugins and extensions, just add/remove according to your needs.","title":"Documentation"},{"location":"tools_included/documentation/#documentation","text":"For building documentation we use mkdocs with the material theme. The material theme already includes several plugins and customizations for mkdocs , you can check the material reference and the PyMdown Extensions on how to get your documentation to the next level \ud83d\ude80. We also packed it with mkdocstrings which allows you automatically create documentation from your source code. The mkdocs.yml file already comes pre-filled with some plugins and extensions, just add/remove according to your needs.","title":"Documentation"},{"location":"tools_included/formatters/","text":"Formatters \u00b6 To make sure all our code follows the same format without much effort, we can leverage some tools to do it for us. By default the project is configured to use a line length of 100 . ruff \u00b6 We use ruff to format our code. Check configs on pyproject.toml . sqlfluff \u00b6 We use sqlfluff to format our SQL code. Check configs on pyproject.toml .","title":"Formatters"},{"location":"tools_included/formatters/#formatters","text":"To make sure all our code follows the same format without much effort, we can leverage some tools to do it for us. By default the project is configured to use a line length of 100 .","title":"Formatters"},{"location":"tools_included/formatters/#ruff","text":"We use ruff to format our code. Check configs on pyproject.toml .","title":"ruff"},{"location":"tools_included/formatters/#sqlfluff","text":"We use sqlfluff to format our SQL code. Check configs on pyproject.toml .","title":"sqlfluff"},{"location":"tools_included/github_actions/","text":"GitHub Actions \u00b6 The project bundles some workflows templates and some additional GitHub utils. dependabot.yml : to automatically check if your python dependencies and github actions are up to date labeler.yml : configuration to automatically add labels to your PR\u2019s based on the changed files workflows: build_docker.yaml : template workflow to build and push images to ghcr.io ci.yaml : runs linters and tests main.yaml : main cicd workflow pr-lint.yaml : adds labels to your PR based on configuration and make sure PR title follows defined convention","title":"Github Actions"},{"location":"tools_included/github_actions/#github-actions","text":"The project bundles some workflows templates and some additional GitHub utils. dependabot.yml : to automatically check if your python dependencies and github actions are up to date labeler.yml : configuration to automatically add labels to your PR\u2019s based on the changed files workflows: build_docker.yaml : template workflow to build and push images to ghcr.io ci.yaml : runs linters and tests main.yaml : main cicd workflow pr-lint.yaml : adds labels to your PR based on configuration and make sure PR title follows defined convention","title":"GitHub Actions"},{"location":"tools_included/ide_settings/","text":"IDE Settings \u00b6 When you generate the project, the project will generate config file(s) to help set up your IDE to use some development tool. VSCode \u00b6 A settings.json file will be generated: .vscode/settings.json { \"python.defaultInterpreterPath\" : \"${workspaceFolder}/.venv/bin/python3\" , \"python.terminal.activateEnvironment\" : true , // Linters/formatters \"ruff.path\" : [ \"${workspaceFolder}/.venv/bin/ruff\" ], \"ruff.lint.args\" : [ \"--config=${workspaceFolder}/pyproject.toml\" ], \"ruff.format.args\" : [ \"--config=${workspaceFolder}/pyproject.toml\" ], \"mypy-type-checker.args\" : [ \"--config-file=${workspaceFolder}/pyproject.toml\" ], \"sqlfluff.config\" : \"${workspaceFolder}/pyproject.toml\" , \"sqlfluff.executablePath\" : \"${workspaceFolder}/.venv/bin/sqlfluff\" , // Tests \"python.testing.pytestEnabled\" : true , \"python.testing.pytestPath\" : \"${workspaceFolder}/.venv/bin/pytest\" , \"python.testing.pytestArgs\" : [], // Vertical lines num characters \"editor.rulers\" : [ 120 ], \"[python]\" : { \"editor.formatOnSave\" : true , \"editor.defaultFormatter\" : \"charliermarsh.ruff\" , \"editor.codeActionsOnSave\" : { \"source.fixAll\" : true }, \"editor.rulers\" : [ { \"column\" : 80 , \"color\" : \"#40824c38\" }, { \"column\" : 100 , \"color\" : \"#80808067\" }, { \"column\" : 120 , \"color\" : \"#ff010168\" }, ] }, \"[jsonc]\" : { \"editor.rulers\" : [] }, \"[git-commit]\" : { \"editor.rulers\" : [ 50 , 72 ] }, \"[markdown]\" : { \"editor.rulers\" : [ 120 , ] }, \"files.associations\" : { \".gitmessage\" : \"git-commit\" , \"*.toml\" : \"toml\" , }, } This is can be useful as it will automatically set up linters, formatters and tests to give you feedback inline. It\u2019ll also set up your python interpreter to use the environment created by poetry , and automatically activate this environment when you open a new shell. If you already have your setup and don\u2019t want to use these settings, feel free to modify/delete them. As this file can vary from developer to developer, depending on the extensions and customizations each wants per project, this file is added on the .gitignore , so it will only be available/generated upon project creation.","title":"IDE Settings"},{"location":"tools_included/ide_settings/#ide-settings","text":"When you generate the project, the project will generate config file(s) to help set up your IDE to use some development tool.","title":"IDE Settings"},{"location":"tools_included/ide_settings/#vscode","text":"A settings.json file will be generated: .vscode/settings.json { \"python.defaultInterpreterPath\" : \"${workspaceFolder}/.venv/bin/python3\" , \"python.terminal.activateEnvironment\" : true , // Linters/formatters \"ruff.path\" : [ \"${workspaceFolder}/.venv/bin/ruff\" ], \"ruff.lint.args\" : [ \"--config=${workspaceFolder}/pyproject.toml\" ], \"ruff.format.args\" : [ \"--config=${workspaceFolder}/pyproject.toml\" ], \"mypy-type-checker.args\" : [ \"--config-file=${workspaceFolder}/pyproject.toml\" ], \"sqlfluff.config\" : \"${workspaceFolder}/pyproject.toml\" , \"sqlfluff.executablePath\" : \"${workspaceFolder}/.venv/bin/sqlfluff\" , // Tests \"python.testing.pytestEnabled\" : true , \"python.testing.pytestPath\" : \"${workspaceFolder}/.venv/bin/pytest\" , \"python.testing.pytestArgs\" : [], // Vertical lines num characters \"editor.rulers\" : [ 120 ], \"[python]\" : { \"editor.formatOnSave\" : true , \"editor.defaultFormatter\" : \"charliermarsh.ruff\" , \"editor.codeActionsOnSave\" : { \"source.fixAll\" : true }, \"editor.rulers\" : [ { \"column\" : 80 , \"color\" : \"#40824c38\" }, { \"column\" : 100 , \"color\" : \"#80808067\" }, { \"column\" : 120 , \"color\" : \"#ff010168\" }, ] }, \"[jsonc]\" : { \"editor.rulers\" : [] }, \"[git-commit]\" : { \"editor.rulers\" : [ 50 , 72 ] }, \"[markdown]\" : { \"editor.rulers\" : [ 120 , ] }, \"files.associations\" : { \".gitmessage\" : \"git-commit\" , \"*.toml\" : \"toml\" , }, } This is can be useful as it will automatically set up linters, formatters and tests to give you feedback inline. It\u2019ll also set up your python interpreter to use the environment created by poetry , and automatically activate this environment when you open a new shell. If you already have your setup and don\u2019t want to use these settings, feel free to modify/delete them. As this file can vary from developer to developer, depending on the extensions and customizations each wants per project, this file is added on the .gitignore , so it will only be available/generated upon project creation.","title":"VSCode"},{"location":"tools_included/linters/","text":"Linters \u00b6 ruff \u00b6 Linter for our python code. This comes bundled with several plugins enabled. You can check them and respective configs on pyproject.toml . For extra documentation on how to add and/or configure plugins, please see the ruff documentation. mypy \u00b6 Used for static type checking in Python. The configurations are kinda strict, to enforce type annotations. Check configs on pyproject.toml . shellcheck \u00b6 Used to lint our shell scripts.","title":"Linters"},{"location":"tools_included/linters/#linters","text":"","title":"Linters"},{"location":"tools_included/linters/#ruff","text":"Linter for our python code. This comes bundled with several plugins enabled. You can check them and respective configs on pyproject.toml . For extra documentation on how to add and/or configure plugins, please see the ruff documentation.","title":"ruff"},{"location":"tools_included/linters/#mypy","text":"Used for static type checking in Python. The configurations are kinda strict, to enforce type annotations. Check configs on pyproject.toml .","title":"mypy"},{"location":"tools_included/linters/#shellcheck","text":"Used to lint our shell scripts.","title":"shellcheck"},{"location":"tools_included/logging/","text":"Logging \u00b6 For logging, we\u2019re using a centralized file ( logging_config.yaml ), under <my_project_name>/src/configs/<your_environment> (where your_environment is defined by the env var APP_ENV , defaults to local ). In this file, you can define your log formatters , filters , handlers and your root and specific loggers (including from other packages). To better understand how to configure it, you can check the logging-cookbook . Custom logging objects \u00b6 We can define custom logging object to be used on our configurations file. To defined these object, the implementation should be in the src/<my_package_name>/logging_setup.py file to be properly loaded. Handlers \u00b6 For the native handlers please check logging.handlers . Other handler you might find useful is watchtower , which allows you send your logs to AWS CloudWatch Logs. Filters \u00b6 To define a custom filter you\u2019ll need to create a subclass of logging.Filter inside src/<my_package_name>/logging_setup.py . One example of a filter could be: class InfoFilter ( logging . Filter ): \"\"\"Example of a simple logger filter, to only select logs with level INFO.\"\"\" def filter ( self , record : logging . LogRecord ) -> bool : \"\"\"Determine if the specified record is to be logged. Returns True if the record should be logged, or False otherwise. If deemed appropriate, the record may be modified in-place. Args: record (logging.LogRecord): log record Returns: bool: the filter predicate execution \"\"\" return record . levelno == logging . INFO Then to use the filter, on you logging_config.yaml , configure it like: formatters : standard : format : \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\" filters : infoFilter : () : <my_package_name>.logging_setup.InfoFilter handlers : console : class : logging.StreamHandler level : DEBUG formatter : standard stream : ext://sys.stdout cloudwatch : class : watchtower.CloudWatchLogHandler level : INFO formatter : standard log_group : /custom/apps/<my_package_name> stream_name : app log_group_retention_days : 90 info_file : class : logging.handlers.RotatingFileHandler level : INFO formatter : standard filename : /tmp/info.log maxBytes : 10485760 backupCount : 20 filters : [ infoFilter ] # example of a log filter encoding : utf8 In this case, the handler info_file will only contains INFO logging records. Loggers \u00b6 Note: when defining loggers be mindful about the parameter propagate . As a rule of thumb, if you attach an handler to a logger, you will typically want to set this parameter to false, if not, the log record will be passed to the handlers of higher level (ancestor) loggers. For instances: root : level : NOTSET # if set, this will be the default logging level for all packages not cover in loggers section handlers : [ console ] propagate : no loggers : <my_package_name> : level : WARNING handlers : [ console ] propagate : no <my_package_name>.some_package_a : level : DEBUG handlers : [ info_file ] propagate : no <my_package_name>.some_package_b : level : DEBUG handlers : [ info_file ] <my_package_name>.some_package_c : level : INFO <my_package_name>.some_package_d : # level: NOTSET handlers : [ info_file ] Lets see what\u2019s happening in there 4 last loggers: <my_package_name>.some_package_a : log records are send only to info_file handler, with level DEBUG <my_package_name>.some_package_b : log records are send to info_file and console (ancestor logger) handlers, with level DEBUG <my_package_name>.some_package_c : log records are send only to console (ancestor logger) handler, with level INFO <my_package_name>.some_package_d : log records are send to info_file and console (ancestor logger) handlers, with level WARNING (ancestor logger level)","title":"Logging"},{"location":"tools_included/logging/#logging","text":"For logging, we\u2019re using a centralized file ( logging_config.yaml ), under <my_project_name>/src/configs/<your_environment> (where your_environment is defined by the env var APP_ENV , defaults to local ). In this file, you can define your log formatters , filters , handlers and your root and specific loggers (including from other packages). To better understand how to configure it, you can check the logging-cookbook .","title":"Logging"},{"location":"tools_included/logging/#custom-logging-objects","text":"We can define custom logging object to be used on our configurations file. To defined these object, the implementation should be in the src/<my_package_name>/logging_setup.py file to be properly loaded.","title":"Custom logging objects"},{"location":"tools_included/logging/#handlers","text":"For the native handlers please check logging.handlers . Other handler you might find useful is watchtower , which allows you send your logs to AWS CloudWatch Logs.","title":"Handlers"},{"location":"tools_included/logging/#filters","text":"To define a custom filter you\u2019ll need to create a subclass of logging.Filter inside src/<my_package_name>/logging_setup.py . One example of a filter could be: class InfoFilter ( logging . Filter ): \"\"\"Example of a simple logger filter, to only select logs with level INFO.\"\"\" def filter ( self , record : logging . LogRecord ) -> bool : \"\"\"Determine if the specified record is to be logged. Returns True if the record should be logged, or False otherwise. If deemed appropriate, the record may be modified in-place. Args: record (logging.LogRecord): log record Returns: bool: the filter predicate execution \"\"\" return record . levelno == logging . INFO Then to use the filter, on you logging_config.yaml , configure it like: formatters : standard : format : \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\" filters : infoFilter : () : <my_package_name>.logging_setup.InfoFilter handlers : console : class : logging.StreamHandler level : DEBUG formatter : standard stream : ext://sys.stdout cloudwatch : class : watchtower.CloudWatchLogHandler level : INFO formatter : standard log_group : /custom/apps/<my_package_name> stream_name : app log_group_retention_days : 90 info_file : class : logging.handlers.RotatingFileHandler level : INFO formatter : standard filename : /tmp/info.log maxBytes : 10485760 backupCount : 20 filters : [ infoFilter ] # example of a log filter encoding : utf8 In this case, the handler info_file will only contains INFO logging records.","title":"Filters"},{"location":"tools_included/logging/#loggers","text":"Note: when defining loggers be mindful about the parameter propagate . As a rule of thumb, if you attach an handler to a logger, you will typically want to set this parameter to false, if not, the log record will be passed to the handlers of higher level (ancestor) loggers. For instances: root : level : NOTSET # if set, this will be the default logging level for all packages not cover in loggers section handlers : [ console ] propagate : no loggers : <my_package_name> : level : WARNING handlers : [ console ] propagate : no <my_package_name>.some_package_a : level : DEBUG handlers : [ info_file ] propagate : no <my_package_name>.some_package_b : level : DEBUG handlers : [ info_file ] <my_package_name>.some_package_c : level : INFO <my_package_name>.some_package_d : # level: NOTSET handlers : [ info_file ] Lets see what\u2019s happening in there 4 last loggers: <my_package_name>.some_package_a : log records are send only to info_file handler, with level DEBUG <my_package_name>.some_package_b : log records are send to info_file and console (ancestor logger) handlers, with level DEBUG <my_package_name>.some_package_c : log records are send only to console (ancestor logger) handler, with level INFO <my_package_name>.some_package_d : log records are send to info_file and console (ancestor logger) handlers, with level WARNING (ancestor logger level)","title":"Loggers"},{"location":"tools_included/makefile/","text":"Makefile \u00b6 The project includes a Makefile to help automate some tasks. These tasks can be grouped into sections. System Dependencies \u00b6 To check if you have all the required tools: make doctor Project Dependencies \u00b6 Tasks that help you set up your environment (create a virtual environment, install packages and tools\u2026) install \u00b6 make install This will should be the first command to prepare your environment. It will set up several things: create a .python-version file, that will tell pyenv which python version to use. Behind the scenes, it will look at the python version you select during project generation (which is stored in the variable PYTHON_VERSION on your Makefile), and will search for the latest version available and use it (make sure to have pyenv up to date) poetry will use this version and create your virtual environment on the project root ( .venv folder), and then install the project core , dev and test dependencies if there\u2019s no git initialization in the project, it will perform a git init , if there is, it will be skipped next, it will install the pre-commit hooks and install the git message template install-jupyter \u00b6 make install-jupyter Install the dependencies to run jupyter notebooks. This target is only available if selected in the project setup ( include_notebooks ). install-docs \u00b6 make install-docs Install the dependencies to build documentation. This target is only available if selected in the project setup ( include_docs ). requirements.txt \u00b6 make requirements.txt This will update/generate the project requirements.txt , based on the installed dependencies from poetry. Checks \u00b6 Tasks to run linters, formatters and python dependencies vulnerabilities scanner format \u00b6 make format Runs all formatters: sqlfluff (SQL) and ruff (fix-only) on your src and tests directory. You can also run each check individually: make format-sqlfluff or make format-ruff . check-packages \u00b6 make check-packages Runs checks on packages: - Checks the validity of the pyproject.toml file - Verify installed packages have compatible dependencies - Run safety check to find vulnerabilities in Python dependencies lint \u00b6 make -k lint Runs sqlfluff , mypy and ruff on your src and tests directory, and shellcheck on shell files. You can also run each check individually: make lint-mypy , make lint-ruff or make lint-shellcheck . check \u00b6 make -k check Runs both check-packages and lint targets. pre-commit \u00b6 make pre-commit Runs the pre-commit checks on all files. Tests \u00b6 Tasks related to testing. test \u00b6 make test Runs the tests with pytest . As we\u2019re using pytest-randomly to shuffle the tests, if the last run of the tests fails, it will run the test with the same random seed first, and then, if the tests pass, it will run with a new one. read-coverage \u00b6 make read-coverage Opens the coverage report for the last pytest run. Documentation \u00b6 Tasks related to documentation. This section is only available if selected in the project setup ( include_docs ). build-docs \u00b6 make build-docs Generate mkdocs documentation locally. The first this target is executed, it will run the target install-docs before. docs \u00b6 make docs Build docs and serve them. Build \u00b6 Tasks related to builds. dist \u00b6 make dist Builds the package, as a tarball and a wheel. Cleanup \u00b6 Tasks to clean up. clean \u00b6 make clean Delete all generated and temporary files. clean-all \u00b6 make clean-all Delete the virtual environment and all generated and temporary files. Docker \u00b6 Tasks related to docker. This section is only available if selected in the project setup ( include_docker ). build-docker \u00b6 make build-docker Build the docker image. run-docker \u00b6 make run-docker Run the docker container for the built image. Other Tasks \u00b6 ci \u00b6 make -k ci Run targets format , check , test and build-docs (if selected) jupyter \u00b6 make jupyter Run jupyter notebooks on the notebooks directory (it will be created if it doesn\u2019t exist). The first this target is executed, it will run the target install-jupyter before.","title":"Makefile"},{"location":"tools_included/makefile/#makefile","text":"The project includes a Makefile to help automate some tasks. These tasks can be grouped into sections.","title":"Makefile"},{"location":"tools_included/makefile/#system-dependencies","text":"To check if you have all the required tools: make doctor","title":"System Dependencies"},{"location":"tools_included/makefile/#project-dependencies","text":"Tasks that help you set up your environment (create a virtual environment, install packages and tools\u2026)","title":"Project Dependencies"},{"location":"tools_included/makefile/#install","text":"make install This will should be the first command to prepare your environment. It will set up several things: create a .python-version file, that will tell pyenv which python version to use. Behind the scenes, it will look at the python version you select during project generation (which is stored in the variable PYTHON_VERSION on your Makefile), and will search for the latest version available and use it (make sure to have pyenv up to date) poetry will use this version and create your virtual environment on the project root ( .venv folder), and then install the project core , dev and test dependencies if there\u2019s no git initialization in the project, it will perform a git init , if there is, it will be skipped next, it will install the pre-commit hooks and install the git message template","title":"install"},{"location":"tools_included/makefile/#install-jupyter","text":"make install-jupyter Install the dependencies to run jupyter notebooks. This target is only available if selected in the project setup ( include_notebooks ).","title":"install-jupyter"},{"location":"tools_included/makefile/#install-docs","text":"make install-docs Install the dependencies to build documentation. This target is only available if selected in the project setup ( include_docs ).","title":"install-docs"},{"location":"tools_included/makefile/#requirementstxt","text":"make requirements.txt This will update/generate the project requirements.txt , based on the installed dependencies from poetry.","title":"requirements.txt"},{"location":"tools_included/makefile/#checks","text":"Tasks to run linters, formatters and python dependencies vulnerabilities scanner","title":"Checks"},{"location":"tools_included/makefile/#format","text":"make format Runs all formatters: sqlfluff (SQL) and ruff (fix-only) on your src and tests directory. You can also run each check individually: make format-sqlfluff or make format-ruff .","title":"format"},{"location":"tools_included/makefile/#check-packages","text":"make check-packages Runs checks on packages: - Checks the validity of the pyproject.toml file - Verify installed packages have compatible dependencies - Run safety check to find vulnerabilities in Python dependencies","title":"check-packages"},{"location":"tools_included/makefile/#lint","text":"make -k lint Runs sqlfluff , mypy and ruff on your src and tests directory, and shellcheck on shell files. You can also run each check individually: make lint-mypy , make lint-ruff or make lint-shellcheck .","title":"lint"},{"location":"tools_included/makefile/#check","text":"make -k check Runs both check-packages and lint targets.","title":"check"},{"location":"tools_included/makefile/#pre-commit","text":"make pre-commit Runs the pre-commit checks on all files.","title":"pre-commit"},{"location":"tools_included/makefile/#tests","text":"Tasks related to testing.","title":"Tests"},{"location":"tools_included/makefile/#test","text":"make test Runs the tests with pytest . As we\u2019re using pytest-randomly to shuffle the tests, if the last run of the tests fails, it will run the test with the same random seed first, and then, if the tests pass, it will run with a new one.","title":"test"},{"location":"tools_included/makefile/#read-coverage","text":"make read-coverage Opens the coverage report for the last pytest run.","title":"read-coverage"},{"location":"tools_included/makefile/#documentation","text":"Tasks related to documentation. This section is only available if selected in the project setup ( include_docs ).","title":"Documentation"},{"location":"tools_included/makefile/#build-docs","text":"make build-docs Generate mkdocs documentation locally. The first this target is executed, it will run the target install-docs before.","title":"build-docs"},{"location":"tools_included/makefile/#docs","text":"make docs Build docs and serve them.","title":"docs"},{"location":"tools_included/makefile/#build","text":"Tasks related to builds.","title":"Build"},{"location":"tools_included/makefile/#dist","text":"make dist Builds the package, as a tarball and a wheel.","title":"dist"},{"location":"tools_included/makefile/#cleanup","text":"Tasks to clean up.","title":"Cleanup"},{"location":"tools_included/makefile/#clean","text":"make clean Delete all generated and temporary files.","title":"clean"},{"location":"tools_included/makefile/#clean-all","text":"make clean-all Delete the virtual environment and all generated and temporary files.","title":"clean-all"},{"location":"tools_included/makefile/#docker","text":"Tasks related to docker. This section is only available if selected in the project setup ( include_docker ).","title":"Docker"},{"location":"tools_included/makefile/#build-docker","text":"make build-docker Build the docker image.","title":"build-docker"},{"location":"tools_included/makefile/#run-docker","text":"make run-docker Run the docker container for the built image.","title":"run-docker"},{"location":"tools_included/makefile/#other-tasks","text":"","title":"Other Tasks"},{"location":"tools_included/makefile/#ci","text":"make -k ci Run targets format , check , test and build-docs (if selected)","title":"ci"},{"location":"tools_included/makefile/#jupyter","text":"make jupyter Run jupyter notebooks on the notebooks directory (it will be created if it doesn\u2019t exist). The first this target is executed, it will run the target install-jupyter before.","title":"jupyter"},{"location":"tools_included/poetry/","text":"Poetry \u00b6 Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. The project uses poetry to manage your dependencies in a deterministic way, to make sure everyone uses the same dependencies versions (and the dependencies of the dependencies). Dependency groups \u00b6 The project organizes some dependencies into groups, so you don\u2019t need to install some groups of dependencies unless they\u2019re required. The predefined groups of dependencies are: your core project dependencies dev : related to development tasks (formatter, linters, pre-commit) tests : to run tests docs : to build documentation jupyter : to run jupyter notebooks and formatter and linters for notebooks To learn how to manage these groups, please refer to poetry dependency groups documentation Basic usage \u00b6 Here we describe some basic usage for poetry, for more details and additional available commands, please refer to the documentation . Install dependencies \u00b6 Install the required dependencies: poetry install If there\u2019s no poetry.lock Poetry simply resolves all dependencies listed in your pyproject.toml file and downloads the latest version of their files. In the end, it will write the poetry.lock . If you already have a poetry.lock file, it will use this file to install the exact versions listed on it. If you also want to install some optional dependency group, let\u2019s say docs , you should run: poetry install --with docs Add new dependencies \u00b6 If you want to add a new dependency to your core dependencies (let\u2019s say django ), run: poetry add django This will find a suitable version constraint and install the package and sub-dependencies If you add to add a dependency to a specific group (let\u2019s say pytest into tests group), run: poetry add pytest --group tests You can also specify a version constraint : poetry add django@4.0.0 In this case, poetry would install the exact specified django version. For more options on how to add dependencies, please check the dependency specification documentation . Update dependencies \u00b6 To update all your dependencies, to the latest allowed versions (according to constraints), run: poetry update If you just want to update some dependencies (let\u2019s say django and pytest ), run: poetry django pytest If you need to update a dependency outside the constraint defined in the pyproject.toml , you\u2019ll have to use the poetry add command. Remove dependencies \u00b6 To remove a dependency (let\u2019s say django ), from your core dependency group run: poetry remove django If it\u2019s in other dependency group (let\u2019s say pytest into tests group), run: poetry remove pytest --group tests Show package details \u00b6 Poetry also allows you to check package details. To check all your required dependencies details run: poetry show If you also want some optional dependency groups (let\u2019s say tests group), run: poetry show --with tests For detailed information on a specific package (let\u2019s say pytest ), run: poetry show pytest you\u2019ll see something like this: name : pytest version : 7.2.0 description : pytest: simple powerful testing with Python dependencies - attrs >=19.2.0 - colorama * - exceptiongroup >=1.0.0rc8 - iniconfig * - packaging * - pluggy >=0.12,<2.0 - tomli >=1.0.0 required by - pytest-clarity >=3.5.0 - pytest-cookies >=3.3.0 - pytest-cov >=4.6 Running commands using your virtual environment \u00b6 You can make sure you\u2019re running commands inside your project virtual environment in 2 ways: poetry run <my_command> This will execute <my_command> from your virtual environment, and return to the environment where you were running. Or you can also run: poetry shell # (1) Note that this command starts a new shell and activates the virtual environment. As such, exit should be used to properly exit the shell and the virtual environment instead of deactivate . This spawns a shell, according to the $SHELL environment variable, within the virtual environment. Then you can run your command inside the provisioned shell.","title":"Poetry"},{"location":"tools_included/poetry/#poetry","text":"Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution. The project uses poetry to manage your dependencies in a deterministic way, to make sure everyone uses the same dependencies versions (and the dependencies of the dependencies).","title":"Poetry"},{"location":"tools_included/poetry/#dependency-groups","text":"The project organizes some dependencies into groups, so you don\u2019t need to install some groups of dependencies unless they\u2019re required. The predefined groups of dependencies are: your core project dependencies dev : related to development tasks (formatter, linters, pre-commit) tests : to run tests docs : to build documentation jupyter : to run jupyter notebooks and formatter and linters for notebooks To learn how to manage these groups, please refer to poetry dependency groups documentation","title":"Dependency groups"},{"location":"tools_included/poetry/#basic-usage","text":"Here we describe some basic usage for poetry, for more details and additional available commands, please refer to the documentation .","title":"Basic usage"},{"location":"tools_included/poetry/#install-dependencies","text":"Install the required dependencies: poetry install If there\u2019s no poetry.lock Poetry simply resolves all dependencies listed in your pyproject.toml file and downloads the latest version of their files. In the end, it will write the poetry.lock . If you already have a poetry.lock file, it will use this file to install the exact versions listed on it. If you also want to install some optional dependency group, let\u2019s say docs , you should run: poetry install --with docs","title":"Install dependencies"},{"location":"tools_included/poetry/#add-new-dependencies","text":"If you want to add a new dependency to your core dependencies (let\u2019s say django ), run: poetry add django This will find a suitable version constraint and install the package and sub-dependencies If you add to add a dependency to a specific group (let\u2019s say pytest into tests group), run: poetry add pytest --group tests You can also specify a version constraint : poetry add django@4.0.0 In this case, poetry would install the exact specified django version. For more options on how to add dependencies, please check the dependency specification documentation .","title":"Add new dependencies"},{"location":"tools_included/poetry/#update-dependencies","text":"To update all your dependencies, to the latest allowed versions (according to constraints), run: poetry update If you just want to update some dependencies (let\u2019s say django and pytest ), run: poetry django pytest If you need to update a dependency outside the constraint defined in the pyproject.toml , you\u2019ll have to use the poetry add command.","title":"Update dependencies"},{"location":"tools_included/poetry/#remove-dependencies","text":"To remove a dependency (let\u2019s say django ), from your core dependency group run: poetry remove django If it\u2019s in other dependency group (let\u2019s say pytest into tests group), run: poetry remove pytest --group tests","title":"Remove dependencies"},{"location":"tools_included/poetry/#show-package-details","text":"Poetry also allows you to check package details. To check all your required dependencies details run: poetry show If you also want some optional dependency groups (let\u2019s say tests group), run: poetry show --with tests For detailed information on a specific package (let\u2019s say pytest ), run: poetry show pytest you\u2019ll see something like this: name : pytest version : 7.2.0 description : pytest: simple powerful testing with Python dependencies - attrs >=19.2.0 - colorama * - exceptiongroup >=1.0.0rc8 - iniconfig * - packaging * - pluggy >=0.12,<2.0 - tomli >=1.0.0 required by - pytest-clarity >=3.5.0 - pytest-cookies >=3.3.0 - pytest-cov >=4.6","title":"Show package details"},{"location":"tools_included/poetry/#running-commands-using-your-virtual-environment","text":"You can make sure you\u2019re running commands inside your project virtual environment in 2 ways: poetry run <my_command> This will execute <my_command> from your virtual environment, and return to the environment where you were running. Or you can also run: poetry shell # (1) Note that this command starts a new shell and activates the virtual environment. As such, exit should be used to properly exit the shell and the virtual environment instead of deactivate . This spawns a shell, according to the $SHELL environment variable, within the virtual environment. Then you can run your command inside the provisioned shell.","title":"Running commands using your virtual environment"},{"location":"tools_included/pre-commit/","text":"pre-commit \u00b6 Having pre-commit hooks allows one to run different types of checks in different stages. In this project, we\u2019re only installing by default hooks for pre-commit , pre-push and commit-msg . So if you want to add other stages, please make sure to install them before. If you have any doubts you can always check the documentation . Following is a list with the included hooks: commitizen : hook to verify your commit message follows the conventional commits (there\u2019s a .gitmessage template that is installed with your project that help you with that) trailing-whitespace : remove any trailing whitespace (ignores line breaks in markdown files) end-of-file-fixer : make sure all files ends with a new line check-yaml : attempts to load all yaml files to verify syntax check-symlinks : checks for symlinks which do not point to anything check-toml : attempts to load all TOML files to verify syntax check-added-large-files : prevent giant files from being committed (default: 1000kB ) check-packages : runs safety check linter ruff : runs ruff linter mypy : runs mypy type checker shellcheck : runs shellcheck linter","title":"pre-commit"},{"location":"tools_included/pre-commit/#pre-commit","text":"Having pre-commit hooks allows one to run different types of checks in different stages. In this project, we\u2019re only installing by default hooks for pre-commit , pre-push and commit-msg . So if you want to add other stages, please make sure to install them before. If you have any doubts you can always check the documentation . Following is a list with the included hooks: commitizen : hook to verify your commit message follows the conventional commits (there\u2019s a .gitmessage template that is installed with your project that help you with that) trailing-whitespace : remove any trailing whitespace (ignores line breaks in markdown files) end-of-file-fixer : make sure all files ends with a new line check-yaml : attempts to load all yaml files to verify syntax check-symlinks : checks for symlinks which do not point to anything check-toml : attempts to load all TOML files to verify syntax check-added-large-files : prevent giant files from being committed (default: 1000kB ) check-packages : runs safety check linter ruff : runs ruff linter mypy : runs mypy type checker shellcheck : runs shellcheck linter","title":"pre-commit"},{"location":"tools_included/tests/","text":"Tests \u00b6 For testing, we use pytest with some predefined configs (you can check it on pyproject.toml file). Additionaly, we\u2019re also packing pytest with some plugins: pytest-cov : produces coverage reports pytest-clarity : coloured diff output pytest-randomly : randomly order tests and controls random.seed pytest-env : enables you to set environment variables in the pytest config file xdoctest : allows executing tests in docstrings","title":"Tests"},{"location":"tools_included/tests/#tests","text":"For testing, we use pytest with some predefined configs (you can check it on pyproject.toml file). Additionaly, we\u2019re also packing pytest with some plugins: pytest-cov : produces coverage reports pytest-clarity : coloured diff output pytest-randomly : randomly order tests and controls random.seed pytest-env : enables you to set environment variables in the pytest config file xdoctest : allows executing tests in docstrings","title":"Tests"}]}